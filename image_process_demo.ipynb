{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b75870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623b9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psf2otf_torch(psf: Tensor, out_h: int, out_w: int):\n",
    "    added_K = False\n",
    "    if psf.dim() == 3:\n",
    "        psf = psf[None, :, :, :]\n",
    "        added_K = True\n",
    "\n",
    "    K, H, W, C = psf.shape\n",
    "    device = psf.device\n",
    "    dtype = psf.dtype\n",
    "\n",
    "    def ifftshift2d(x):\n",
    "        return torch.roll(torch.roll(x, shifts=(-H // 2), dims=1),\n",
    "                          shifts=(-W // 2),\n",
    "                          dims=2)\n",
    "\n",
    "    psf_shifted = ifftshift2d(psf)\n",
    "\n",
    "    otf_pad = torch.zeros((K, out_h, out_w, C), device=device, dtype=dtype)\n",
    "    otf_pad[:, :H, :W, :] = psf_shifted\n",
    "\n",
    "    otf = torch.fft.fft2(otf_pad, dim=(1, 2))\n",
    "\n",
    "    if added_K:\n",
    "        otf = otf[0]\n",
    "\n",
    "    return otf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0b281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_psf_conv_torch(img: Tensor,\n",
    "                       psf: Tensor,\n",
    "                       otf: Tensor=None,\n",
    "                       adjoint: bool = False,\n",
    "                       return_otf: bool = False):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "    y(blur images):[B,K,H,W,C]\n",
    "    otf:psf-->otf,as big as after padding img\n",
    "    \"\"\"\n",
    "    added_B = False\n",
    "    if img.dim() == 3:\n",
    "        img = img[None, :, :, :]\n",
    "        added_B = True\n",
    "    B, H, W, C = img.shape\n",
    "\n",
    "    added_K = False\n",
    "    if psf.dim() == 3:\n",
    "        psf = psf[None,:,:,:]\n",
    "        added_K = True\n",
    "    K,h,w,C_psf = psf.shape\n",
    "    assert C_psf == C\n",
    "\n",
    "    device = img.device\n",
    "    fdtype = img.real.dtype if img.is_complex() else img.dtype\n",
    "\n",
    "    H2,W2 = 2*H,2*W\n",
    "    img_pad = torch.zeros((B,H2,W2,C),device=device,dtype=fdtype)\n",
    "    img_pad[:,:H,:W,:] = img\n",
    "\n",
    "    #prepare OTF\n",
    "    if otf is None:\n",
    "        otf = psf2otf_torch(psf.to(device=device,dtype=fdtype),H2,W2)\n",
    "    if otf.dim() == 3:\n",
    "        otf = otf.unsqueeze(0)\n",
    "\n",
    "    IMG_F = torch.fft.fft2(img_pad,dim=(1,2))\n",
    "    OTF_F = otf.to(device=IMG_F.device,dtype=IMG_F.dtype)\n",
    "    if adjoint:\n",
    "        OTF_F = torch.conj(OTF_F)\n",
    "\n",
    "    # for broadcasting\n",
    "    Y_F = IMG_F[:,None,:,:,:] * OTF_F[None,:,:,:,:]\n",
    "\n",
    "    y_big = torch.fft.ifft2(Y_F,dim=(2,3)).real\n",
    "\n",
    "    blurred_img = y_big[:,:,:H,:W,:]\n",
    "\n",
    "    if added_K:\n",
    "        blurred_img = blurred_img[:,0,:,:,:]\n",
    "    if added_B:\n",
    "        blurred_img = blurred_img[0,:,:,:,:]\n",
    "\n",
    "    if return_otf:\n",
    "        return blurred_img,otf\n",
    "    return blurred_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c9a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H, W, C = 1, 256, 256, 3\n",
    "K, h, w     = 5, 33, 33\n",
    "img = torch.rand(B, H, W, C, device='cuda')\n",
    "psf = torch.rand(K, h, w, C, device='cuda')\n",
    "psf = psf / (psf.sum(dim=(1,2), keepdim=True) + 1e-8)  # 单位能量\n",
    "\n",
    "y = img_psf_conv_torch(img, psf)         # [B,K,H,W,C] 模糊体\n",
    "# 如果下一次还用同一套 psf：\n",
    "y2, cached_otf = img_psf_conv_torch(img, psf, return_otf=True)\n",
    "y3 = img_psf_conv_torch(img, psf, otf=cached_otf)       # 复用 OTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f13317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def depthmap_to_layeralpha_torch(depthmap: Tensor, z_bins: Tensor, mode: str):\n",
    "#     added_B = False\n",
    "#     if depthmap.dim() == 3:\n",
    "#         depthmap = depthmap[None, :, :, :]\n",
    "#         added_B = True\n",
    "\n",
    "#     B, H, W, _ = depthmap.shape\n",
    "#     z: Tensor = depthmap[:, :, :, 0]\n",
    "#     device = z.device\n",
    "#     dtype = z.dtype\n",
    "\n",
    "#     if torch.all(z_bins[1:] - z_bins[:-1] >= 0):\n",
    "#         zb_sorted = z_bins\n",
    "#         rev_idx = None\n",
    "#     elif torch.all(z_bins[1:] - z_bins[:-1] <= 0):\n",
    "#         zb_sorted = torch.flip(z_bins, dims=[0])\n",
    "#         rev_idx = torch.arange(z_bins.numel() - 1, -1, -1, device=device)\n",
    "#     else:\n",
    "#         raise ValueError(\"z_bins are not sorted\")\n",
    "\n",
    "#     K = zb_sorted.numel()\n",
    "\n",
    "#     zc = z.clamp(min=zb_sorted[0], max=zb_sorted[-1])\n",
    "\n",
    "#     alpha = torch.zeros((B, K, H, W, 1), device=device, dtype=dtype)\n",
    "\n",
    "#     if mode == \"nearest\":\n",
    "#         idx_r = torch.searchsorted(zb_sorted, zc, right=False)\n",
    "#         idx_r = idx_r.clamp(0, K - 1)\n",
    "#         idx_l = (idx_r - 1).clamp(0, K - 1)\n",
    "\n",
    "#         d_l = (zc - zb_sorted[idx_l]).abs()\n",
    "#         d_r = (zb_sorted[idx_r] - zc).abs()\n",
    "#         idx = torch.where((idx_r == idx_l) | (d_l <= d_r), idx_l, idx_r)\n",
    "#         alpha.scatter_(1, idx.unsqueeze(1).unsqueeze(-1), 1.0)\n",
    "\n",
    "#     else:\n",
    "#         idx_r = torch.searchsorted(zb_sorted, zc, right=False)\n",
    "#         idx_r = idx_r.clamp(1, K - 1)\n",
    "#         idx_l = idx_r - 1\n",
    "\n",
    "#         z_l = zb_sorted[idx_l]\n",
    "#         z_r = zb_sorted[idx_r]\n",
    "#         denom = (z_r - z_l).clamp_min(1e-12)\n",
    "\n",
    "#         w_r = (zc - z_l) / denom\n",
    "#         w_l = 1.0 - w_r\n",
    "\n",
    "#         alpha.scatter_(1, idx_l.unsqueeze(1).unsqueeze(-1), w_l.unsqueeze(1).unsqueeze(-1))\n",
    "#         alpha.scatter_(1, idx_r.unsqueeze(1).unsqueeze(-1), w_r.unsqueeze(1).unsqueeze(-1))\n",
    "\n",
    "#     if rev_idx is not None:\n",
    "#         alpha = alpha[:, rev_idx, :, :, :]\n",
    "\n",
    "#     if added_B:\n",
    "#         alpha = alpha[0]\n",
    "\n",
    "#     return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2630ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 假设 z_bins = [0.3, 0.5, 1.0] (米)\n",
    "# z_bins = torch.tensor([0.3, 0.5, 1.0], dtype=torch.float32)\n",
    "\n",
    "# # 构造一个 2x2 的深度图\n",
    "# depth = torch.tensor([[0.32, 0.45],\n",
    "#                       [0.80, 1.20]], dtype=torch.float32).view(1,2,2,1)  # [B=1,H=2,W=2,1]\n",
    "\n",
    "# # 最近邻 one-hot\n",
    "# alpha_nn = depthmap_to_layeralpha_torch(depth, z_bins, mode=\"nearest\")   # [1,3,2,2,1]\n",
    "# # 线性分摊\n",
    "# alpha_li = depthmap_to_layeralpha_torch(depth, z_bins, mode=\"linear\")    # [1,3,2,2,1]\n",
    "\n",
    "# # 检查每像素沿 K 维的和（线性模式应≈1）\n",
    "# print(alpha_li.sum(dim=1).squeeze(-1))  # -> 形状 [1,2,2]，应接近全 1\n",
    "# print(\"alpha_li\",alpha_li)\n",
    "# print(\"alpha_nn\",alpha_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef10534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matting_torch(depthmap:Tensor,n_depths:int,binary:bool,eps:float = 1e-8):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    Input:\n",
    "    depthmap:\n",
    "        [B,1,1,H,W] or [B,1,H,W] or [1,H,W]\n",
    "    n_depths:K depth layers\n",
    "    binary:True -> one-hot\n",
    "    Output:\n",
    "    alpha:[B,1,K,H,W]\n",
    "    \"\"\"\n",
    "    x = depthmap\n",
    "    if x.dim() == 3:\n",
    "        x = x.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "    elif x.dim() == 4:\n",
    "        if x.shape[1] != 1:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = x.unsqueeze(2)\n",
    "    elif x.dim() == 5:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"depthmap should be 3D/4D/5D tensor\")\n",
    "    \n",
    "    device,dtype = x.device,x.dtype\n",
    "\n",
    "    x = torch.clamp(x,min=eps,max=1)\n",
    "\n",
    "    d = torch.arange(1,n_depths+1,device=device,dtype=dtype).view(1,1,-1,1,1)\n",
    "    \n",
    "    x_scaled = x * float(n_depths)\n",
    "    diff = d - x_scaled\n",
    "\n",
    "    alpha = torch.zeros_like(diff)\n",
    "\n",
    "    if binary:\n",
    "        logi = (diff >= 0.) & (diff < 1.)\n",
    "        alpha = torch.where(logi,torch.ones_like(diff),torch.zeros_like(diff))\n",
    "    else:\n",
    "        mask_left = (diff > -1.) & (diff <= 0.)\n",
    "        alpha[mask_left] = diff[mask_left] + 1.0\n",
    "        mask_right = (diff > 0.) & (diff <= 1.)\n",
    "        alpha[mask_right] = 1.0\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756cd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthmap_to_layer_depth_torch(depthmap:Tensor,n_depths:int,binary:False):\n",
    "    layered_depth = matting_torch(depthmap,n_depths,binary=binary)\n",
    "    return layered_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0c1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_op_torch(alpha:Tensor):\n",
    "    one_minus = (1 - alpha).clamp(0,1)\n",
    "    T_after = torch.cumprod(one_minus,dim=2)\n",
    "    ones_slice = torch.ones_like(alpha[:,:,0:1,:,:])\n",
    "    T_shift = T_after[:,:,:-1,:,:]\n",
    "    T_before = torch.cat([ones_slice,T_shift],dim=2)\n",
    "    return T_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9340cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_img_torch(img:Tensor,depthmap:Tensor,psfs:Tensor,scene_distances:Tensor): # generate sensor img\n",
    "    occlusion = True\n",
    "    eps = 1e-3\n",
    "    B,H,W,C = img.shape\n",
    "    K = psfs.shape[0]\n",
    "    device = img.device\n",
    "    dtype = img.dtype\n",
    "    depthmap = depthmap.to(device=device,dtype=dtype)\n",
    "    psfs = psfs.to(device=device,dtype=dtype)\n",
    "\n",
    "    # depth mapping\n",
    "    layered_alpha = matting_torch(depthmap,len(scene_distances),binary=True)\n",
    "    layered_alpha_rgb = layered_alpha.repeat(1,C,1,1,1)\n",
    "\n",
    "    # mapping rgb\n",
    "    img_k = img.unsqueeze(1).repeat(1,K,1,1,1)\n",
    "    volume = layered_alpha_rgb.permute(0,2,3,4,1) * img_k\n",
    "    scale = volume.max()\n",
    "    volume = volume / (scale + 1e-12)\n",
    "\n",
    "    # conv in different layers\n",
    "    blurred_volume = torch.zeros_like(volume)\n",
    "    blurred_alpha_rgb = torch.zeros_like(volume)\n",
    "\n",
    "    for k in range(K):\n",
    "        vol_k = volume[:,k]\n",
    "        layered_alpha_k_rgb = layered_alpha_rgb[:,:,k].permute(0,2,3,1)\n",
    "        psf_k = psfs[k:k+1]\n",
    "\n",
    "        blurred_k = img_psf_conv_torch(vol_k,psf_k)[:,0]\n",
    "        blurred_alpha_k = img_psf_conv_torch(layered_alpha_k_rgb,psf_k)[:,0]\n",
    "\n",
    "        blurred_volume[:,k] = blurred_k\n",
    "        blurred_alpha_rgb[:,k] = blurred_alpha_k\n",
    "\n",
    "    cumsum_alpha = torch.flip(torch.cumsum(torch.flip(layered_alpha,dims=[2]),dim=2),dims=[2])\n",
    "    E = torch.zeros((B,K,H,W,C),device=device,dtype=dtype)\n",
    "    for k in range(K):\n",
    "        ca_k = cumsum_alpha[:,0,k]\n",
    "        ca_k_c = ca_k.unsqueeze(-1).repeat(1,1,1,C)\n",
    "        psf_k = psfs[k:k+1]\n",
    "        E_k = img_psf_conv_torch(ca_k_c,psf_k)[:,0]\n",
    "        E[:,k] = E_k\n",
    "    \n",
    "    C_tilde = blurred_volume / (E + eps)\n",
    "    A_tilde = blurred_alpha_rgb / (E + eps)\n",
    "\n",
    "    T_before = over_op_torch(A_tilde)\n",
    "\n",
    "    captimg = (C_tilde * T_before).sum(dim=1)\n",
    "    captimg = captimg * (scale + 1e-12)\n",
    "    volume = volume * (scale + 1e-12)\n",
    "    \n",
    "    if not occlusion:\n",
    "        sensor_stack = torch.zeros_like(volume)\n",
    "        for k in range(K):\n",
    "            captimg = sensor_stack.sum(dim=1)\n",
    "            captimg = captimg * (scale + 1e-12)\n",
    "            volume = volume * (scale + 1e-12)\n",
    "            return captimg,volume\n",
    "        \n",
    "    return captimg,volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_img_torch(img:Tensor,depthmap:Tensor,psfs:Tensor,scene_distances:Tensor): # generate sensor img\n",
    "    occlusion = True\n",
    "    eps = 1e-3\n",
    "    B,H,W,C = img.shape\n",
    "    K = psfs.shape[0]\n",
    "    device = img.device\n",
    "    dtype = img.dtype\n",
    "    depthmap = depthmap.to(device=device,dtype=dtype)\n",
    "    psfs = psfs.to(device=device,dtype=dtype)\n",
    "\n",
    "    # depth mapping\n",
    "    layered_alpha = matting_torch(depthmap,len(scene_distances),binary=True)\n",
    "    layered_alpha_rgb = layered_alpha.repeat(1,C,1,1,1)\n",
    "\n",
    "    # mapping rgb\n",
    "    img_k = img.unsqueeze(1).repeat(1,K,1,1,1)\n",
    "    volume = layered_alpha_rgb.permute(0,2,3,4,1) * img_k\n",
    "    scale = volume.max()\n",
    "    volume = volume / (scale + 1e-12)\n",
    "\n",
    "    # conv in different layers\n",
    "    blurred_volume = torch.zeros_like(volume)\n",
    "    blurred_alpha_rgb = torch.zeros_like(volume)\n",
    "\n",
    "    for k in range(K):\n",
    "        vol_k = volume[:,k]\n",
    "        layered_alpha_k_rgb = layered_alpha_rgb[:,:,k].permute(0,2,3,1)\n",
    "        psf_k = psfs[k:k+1]\n",
    "\n",
    "        blurred_k = img_psf_conv_torch(vol_k,psf_k)[:,0]\n",
    "        blurred_alpha_k = img_psf_conv_torch(layered_alpha_k_rgb,psf_k)[:,0]\n",
    "\n",
    "        blurred_volume[:,k] = blurred_k\n",
    "        blurred_alpha_rgb[:,k] = blurred_alpha_k\n",
    "\n",
    "    cumsum_alpha = torch.flip(torch.cumsum(torch.flip(layered_alpha,dims=[2]),dim=2),dims=[2])\n",
    "    E = torch.zeros((B,K,H,W,C),device=device,dtype=dtype)\n",
    "    for k in range(K):\n",
    "        ca_k = cumsum_alpha[:,0,k]\n",
    "        ca_k_c = ca_k.unsqueeze(-1).repeat(1,1,1,C)\n",
    "        psf_k = psfs[k:k+1]\n",
    "        E_k = img_psf_conv_torch(ca_k_c,psf_k)[:,0]\n",
    "        E[:,k] = E_k\n",
    "    \n",
    "    C_tilde = blurred_volume / (E + eps)\n",
    "    A_tilde = blurred_alpha_rgb / (E + eps)\n",
    "\n",
    "    T_before = over_op_torch(A_tilde)\n",
    "\n",
    "    captimg = (C_tilde * T_before).sum(dim=1)\n",
    "    captimg = captimg * (scale + 1e-12)\n",
    "    volume = volume * (scale + 1e-12)\n",
    "    \n",
    "    if not occlusion:\n",
    "        sensor_stack = torch.zeros_like(volume)\n",
    "        for k in range(K):\n",
    "            captimg = sensor_stack.sum(dim=1)\n",
    "            captimg = captimg * (scale + 1e-12)\n",
    "            volume = volume * (scale + 1e-12)\n",
    "            return captimg,volume\n",
    "        \n",
    "    return captimg,volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee087bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_img_torch(img:Tensor,depthmap:Tensor,psfs:Tensor,scene_distances:Tensor): # generate sensor img\n",
    "    occlusion = True\n",
    "    eps = 1e-3\n",
    "    B,H,W,C = img.shape\n",
    "    K = psfs.shape[0]\n",
    "    device = img.device\n",
    "    dtype = img.dtype\n",
    "    depthmap = depthmap.to(device=device,dtype=dtype)\n",
    "    psfs = psfs.to(device=device,dtype=dtype)\n",
    "\n",
    "    # depth mapping\n",
    "    layered_alpha = matting_torch(depthmap,len(scene_distances),binary=True)\n",
    "    layered_alpha_rgb = layered_alpha.repeat(1,C,1,1,1)\n",
    "\n",
    "    # mapping rgb\n",
    "    img_k = img.unsqueeze(1).repeat(1,K,1,1,1)\n",
    "    volume = layered_alpha_rgb.permute(0,2,3,4,1) * img_k\n",
    "    scale = volume.max()\n",
    "    volume = volume / (scale + 1e-12)\n",
    "\n",
    "    # conv in different layers\n",
    "    blurred_volume = torch.zeros_like(volume)\n",
    "    blurred_alpha_rgb = torch.zeros_like(volume)\n",
    "\n",
    "    for k in range(K):\n",
    "        vol_k = volume[:,k]\n",
    "        layered_alpha_k_rgb = layered_alpha_rgb[:,:,k].permute(0,2,3,1)\n",
    "        psf_k = psfs[k:k+1]\n",
    "\n",
    "        blurred_k = img_psf_conv_torch(vol_k,psf_k)[:,0]\n",
    "        blurred_alpha_k = img_psf_conv_torch(layered_alpha_k_rgb,psf_k)[:,0]\n",
    "\n",
    "        blurred_volume[:,k] = blurred_k\n",
    "        blurred_alpha_rgb[:,k] = blurred_alpha_k\n",
    "\n",
    "    cumsum_alpha = torch.flip(torch.cumsum(torch.flip(layered_alpha,dims=[2]),dim=2),dims=[2])\n",
    "    E = torch.zeros((B,K,H,W,C),device=device,dtype=dtype)\n",
    "    for k in range(K):\n",
    "        ca_k = cumsum_alpha[:,0,k]\n",
    "        ca_k_c = ca_k.unsqueeze(-1).repeat(1,1,1,C)\n",
    "        psf_k = psfs[k:k+1]\n",
    "        E_k = img_psf_conv_torch(ca_k_c,psf_k)[:,0]\n",
    "        E[:,k] = E_k\n",
    "    \n",
    "    C_tilde = blurred_volume / (E + eps)\n",
    "    A_tilde = blurred_alpha_rgb / (E + eps)\n",
    "\n",
    "    T_before = over_op_torch(A_tilde)\n",
    "\n",
    "    captimg = (C_tilde * T_before).sum(dim=1)\n",
    "    captimg = captimg * (scale + 1e-12)\n",
    "    volume = volume * (scale + 1e-12)\n",
    "    \n",
    "    if not occlusion:\n",
    "        sensor_stack = torch.zeros_like(volume)\n",
    "        for k in range(K):\n",
    "            captimg = sensor_stack.sum(dim=1)\n",
    "            captimg = captimg * (scale + 1e-12)\n",
    "            volume = volume * (scale + 1e-12)\n",
    "            return captimg,volume\n",
    "        \n",
    "    return captimg,volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94dd8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_noise_torch(x:Tensor,a_poisson:float,b_sqrt:float,clip:Tuple[float,float] = (1e-6,1.0),poisson_max:float=100,sample_poisson:bool = False):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    x:captured image by sensor\n",
    "    a_poisson:float\n",
    "    b_sqrt:float\n",
    "    return:\n",
    "    as same as x's shape(added poisson noise and readout noise)\n",
    "    \"\"\"\n",
    "    device = x.device\n",
    "    dtype = x.dtype\n",
    "    low,high = clip\n",
    "\n",
    "    # -- Shot Noise (Poisson) --\n",
    "    if a_poisson > 0.0:\n",
    "        x_clamped = x.clamp(min=low,max=poisson_max)\n",
    "\n",
    "        if sample_poisson:\n",
    "            with torch.no_grad():\n",
    "                lam = (x_clamped/float(a_poisson)).to(dtype=dtype,device=device)\n",
    "                counts = torch.poisson(lam)\n",
    "            shot = counts*float(a_poisson)\n",
    "        else:\n",
    "            # Poisson(λ) ~ λ + sqrt(λ) * N(0,1)\n",
    "            noise_shot = torch.randn_like(x_clamped) * torch.sqrt((x_clamped*float(a_poisson)).clamp_min(0.0))\n",
    "            shot = x_clamped + noise_shot\n",
    "        y = shot\n",
    "    else:\n",
    "        y = x\n",
    "\n",
    "    # -- Readout Noise (Gaussian) --\n",
    "    if b_sqrt > 0.0:\n",
    "        y = y + torch.randn_like(y)*float(b_sqrt)\n",
    "\n",
    "    # -- clipping --\n",
    "    y = y.clamp(min=low,max=high)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35282c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
